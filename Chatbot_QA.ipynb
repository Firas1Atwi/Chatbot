{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved to Biomedical Research\\D19-1259.pdf\n"
     ]
    }
   ],
   "source": [
    "# URL of the file to download\n",
    "url = \"https://aclanthology.org/D19-1259.pdf\"\n",
    "\n",
    "# Directory to save the file\n",
    "save_directory = \"Biomedical Research\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Construct the file path\n",
    "file_name = url.rpartition(\"/\")[2]\n",
    "file_path = os.path.join(save_directory, file_name)\n",
    "\n",
    "# Download the file\n",
    "urlretrieve(url, file_path)\n",
    "\n",
    "print(f\"File downloaded and saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(\"./Biomedical Research/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'Biomedical Research\\\\D19-1259.pdf', 'page': 0}, page_content='Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\\nand the 9th International Joint Conference on Natural Language Processing, pages 2567–2577,\\nHong Kong, China, November 3–7, 2019.c⃝2019 Association for Computational Linguistics\\n2567\\nPubMedQA: A Dataset for Biomedical Research Question Answering\\nQiao Jin\\nUniversity of Pittsburgh\\nqiao.jin@pitt.edu\\nBhuwan Dhingra\\nCarnegie Mellon University\\nbdhingra@cs.cmu.edu\\nZhengping Liu\\nUniversity of Pittsburgh\\nzliu@pitt.edu\\nWilliam W. Cohen\\nGoogle AI\\nwcohen@google.com\\nXinghua Lu\\nUniversity of Pittsburgh\\nxinghua@pitt.edu\\nAbstract\\nWe introduce PubMedQA, a novel biomedi-\\ncal question answering (QA) dataset collected\\nfrom PubMed abstracts. The task of Pub-\\nMedQA is to answer research questions with\\nyes/no/maybe (e.g.: Do preoperative statins\\nreduce atrial ﬁbrillation after coronary artery\\nbypass grafting?) using the corresponding ab-\\nstracts. PubMedQA has 1k expert-annotated,\\n61.2k unlabeled and 211.3k artiﬁcially gen-\\nerated QA instances. Each PubMedQA in-\\nstance is composed of (1) a question which\\nis either an existing research article title or\\nderived from one, (2) a context which is the\\ncorresponding abstract without its conclusion,\\n(3) a long answer, which is the conclusion of\\nthe abstract and, presumably, answers the re-\\nsearch question, and (4) a yes/no/maybe an-\\nswer which summarizes the conclusion. Pub-\\nMedQA is the ﬁrst QA dataset where rea-\\nsoning over biomedical research texts, espe-\\ncially their quantitative contents, is required\\nto answer the questions. Our best performing\\nmodel, multi-phase ﬁne-tuning of BioBERT\\nwith long answer bag-of-word statistics as\\nadditional supervision, achieves 68.1% accu-\\nracy, compared to single human performance\\nof 78.0% accuracy and majority-baseline of\\n55.2% accuracy, leaving much room for im-\\nprovement. PubMedQA is publicly available\\nat https://pubmedqa.github.io.\\n1 Introduction\\nA long-term goal of natural language understand-\\ning is to build intelligent systems that can reason\\nand infer over natural language. The question an-\\nswering (QA) task, in which models learn how to\\nanswer questions, is often used as a benchmark for\\nquantitatively measuring the reasoning and infer-\\nring abilities of such intelligent systems.\\nWhile many large-scale annotated general do-\\nmain QA datasets have been introduced (Ra-\\njpurkar et al., 2016; Lai et al., 2017; Ko ˇcisk`y\\nQuestion:\\nDo preoperative statins reduce atrial ﬁbrillation after\\ncoronary artery bypass grafting?\\nContext:\\n(Objective) Recent studies have demonstrated that statins\\nhave pleiotropic effects, including anti-inﬂammatory ef-\\nfects and atrial ﬁbrillation (AF) preventive effects [...]\\n(Methods) 221 patients underwent CABG in our hospital\\nfrom 2004 to 2007. 14 patients with preoperative AF and\\n4 patients with concomitant valve surgery [...]\\n(Results) The overall incidence of postoperative AF was\\n26%. Postoperative AF was signiﬁcantly lower in the\\nStatin group compared with the Non-statin group (16%\\nversus 33%, p=0.005). Multivariate analysis demon-\\nstrated that independent predictors of AF [...]\\nLong Answer:\\n(Conclusion) Our study indicated that preoperative statin\\ntherapy seems to reduce AF development after CABG.\\nAnswer: yes\\nFigure 1: An instance (Sakamoto et al., 2011) of Pub-\\nMedQA dataset: Question is the original question title;\\nContext includes the structured abstract except its con-\\nclusive part, which serves as the Long Answer; Human\\nexperts annotated the Answer yes. Supporting fact for\\nthe answer is highlighted.\\net al., 2018; Yang et al., 2018; Kwiatkowski\\net al., 2019), the largest annotated biomedical QA\\ndataset, BioASQ (Tsatsaronis et al., 2015) has less\\nthan 3k training instances, most of which are sim-\\nple factual questions. Some works proposed au-\\ntomatically constructed biomedical QA datasets\\n(Pampari et al., 2018; Pappas et al., 2018; Kim\\net al., 2018), which have much larger sizes. How-\\never, questions of these datasets are mostly fac-\\ntoid, whose answers can be extracted in the con-\\ntexts without much reasoning.\\nIn this paper, we aim at building a biomedi-\\ncal QA dataset which (1) has substantial instances\\nwith some expert annotations and (2) requires rea-\\nsoning over the contexts to answer the questions.\\nFor this, we turn to the PubMed1, a search engine\\nproviding access to over 25 million references of\\n1https://www.ncbi.nlm.nih.gov/pubmed/')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you have 11 document(s) in your data\n",
      "there are 4397 characters in your document\n"
     ]
    }
   ],
   "source": [
    "print(f'you have {len(data)} document(s) in your data')\n",
    "print(f'there are {len(data[0].page_content)} characters in your document')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk your data up into smaller documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size= 700, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 69 documents\n"
     ]
    }
   ],
   "source": [
    "print(f'Now you have {len(texts)} documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'Biomedical Research\\\\D19-1259.pdf', 'page': 0}, page_content='Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\\nand the 9th International Joint Conference on Natural Language Processing, pages 2567–2577,\\nHong Kong, China, November 3–7, 2019.c⃝2019 Association for Computational Linguistics\\n2567\\nPubMedQA: A Dataset for Biomedical Research Question Answering\\nQiao Jin\\nUniversity of Pittsburgh\\nqiao.jin@pitt.edu\\nBhuwan Dhingra\\nCarnegie Mellon University\\nbdhingra@cs.cmu.edu\\nZhengping Liu\\nUniversity of Pittsburgh\\nzliu@pitt.edu\\nWilliam W. Cohen\\nGoogle AI\\nwcohen@google.com\\nXinghua Lu\\nUniversity of Pittsburgh\\nxinghua@pitt.edu\\nAbstract\\nWe introduce PubMedQA, a novel biomedi-\\ncal question answering (QA) dataset collected')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings of your documents to get ready for semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name= \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs = {'device' : 'cpu'},\n",
    "    encode_kwargs = {'normalize_embeddings' : True}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "from pinecone import Pinecone\n",
    "\n",
    "if not os.getenv(\"pcsk_3uJx1J_9EmxrzDCFnF4af1GmiB1Q2HATKJv7pKWrq8E3JfvibkbSEXPVCiDPGXUqD8Fqpd\"):\n",
    "    os.environ[\"pcsk_3uJx1J_9EmxrzDCFnF4af1GmiB1Q2HATKJv7pKWrq8E3JfvibkbSEXPVCiDPGXUqD8Fqpd\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "pinecone_api_key = os.environ.get(\"pcsk_3uJx1J_9EmxrzDCFnF4af1GmiB1Q2HATKJv7pKWrq8E3JfvibkbSEXPVCiDPGXUqD8Fqpd\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"chatbot\"  \n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the Vector Database \n",
    "#### in order to generate the embeddings and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d5856aa8-52f4-4a62-a4f4-8f95b8cf7cdd',\n",
       " 'b19c3d9a-d9c8-4c2d-8f0a-56d64ef1dc24',\n",
       " 'af2a1b88-7ce5-4321-9b13-4840dd678144',\n",
       " '25d5ecc4-06b3-41dc-a08f-f2f26e2003e7',\n",
       " '133277f8-0aa4-459b-a88b-b9765b2b61b7',\n",
       " '69e0e450-d106-4a0e-bcf8-787349417b40',\n",
       " '305eeaf8-d2ef-4d5d-baf1-3df40b0fdb96',\n",
       " 'b9f32d3f-adf6-4bdf-b35e-cc1605e3d6e8',\n",
       " '7e34b80c-a611-4d96-90d2-23dd0529199e',\n",
       " '3d9748ce-6c12-4598-9c79-aac3161d9426',\n",
       " '85c4c725-ab62-4c69-9be3-b9a0d54b7be3',\n",
       " '604d8d00-4dca-4f5c-9e83-d4ff6b4dc7a9',\n",
       " '556a6f04-8b35-45b7-af33-7be94d0da222',\n",
       " '738b6c71-7e05-4cdd-ba45-88b3e4b0fc84',\n",
       " 'e8b455ed-e5b0-460d-a8a7-0844e9f3cde9',\n",
       " '88456c73-d1d3-425e-9b41-65542484332a',\n",
       " '0a9fc7cf-743f-4c29-9894-cffcc5d0e1fd',\n",
       " '8a439ed8-f4c8-4039-a5d6-9fadea7a9a6f',\n",
       " 'af7000d0-2b66-4246-85c3-2cc8400b9d90',\n",
       " 'c4489ea7-01cf-4be6-afde-ebf8f1b3d75e',\n",
       " '9c76eb3d-a20e-4c7b-abb0-ff1b33099748',\n",
       " '566be4c2-786b-4c51-90c5-31673822b621',\n",
       " '08e9e347-aa58-4d50-89ff-cb22e9a2c146',\n",
       " '951b2dfa-e59c-4205-9ac9-d6eeea961062',\n",
       " '7baf223f-b97f-4818-b15d-26ffa5e6e611',\n",
       " 'ce8f2118-3204-42f3-8ddb-a3d631a41f11',\n",
       " 'ef09c975-d3f9-45ca-87ce-7b4f6796b6c8',\n",
       " 'c8e48ac7-e812-475e-895b-d69efb0b932d',\n",
       " '932082dd-ea43-4ebf-a521-74d4afeadc9b',\n",
       " '1c04e838-4437-40cc-bb4c-3c882508d12e',\n",
       " '106f322f-878b-4841-86ee-0e5da4ad1e5b',\n",
       " '7187b4dc-1a1f-4af0-91c2-d389b620cd87',\n",
       " '485f6536-e41f-4f56-bb7e-9af909cc458a',\n",
       " 'bdbc3dd6-f42d-42b0-b821-5f8f0851926f',\n",
       " '7ae65b3e-2ef9-4c53-995b-e68043d1adf8',\n",
       " '0b290ed7-27b8-415a-add1-f24574225a13',\n",
       " '72a9db25-54a0-4d34-99c6-f53f42f7bab5',\n",
       " 'b7d32b3a-2ab7-4efc-9ea4-b7a7525edf73',\n",
       " 'e549adae-b7dd-4b46-9c13-63aca901313b',\n",
       " 'a8f28560-8057-4bc9-891b-6d6f2b35c760',\n",
       " 'ed202a4b-7b9b-4194-8c6f-debec3280742',\n",
       " '7bf2d685-d617-4c11-b50e-42fcb59ccc7d',\n",
       " 'b84ec0da-3fed-4358-8b5d-52aa8fd77dc8',\n",
       " 'd4148684-ae70-46d9-bfc7-36d131733d16',\n",
       " 'f4488da5-b3af-4736-8979-1a4fae94d45d',\n",
       " '49f853a6-5b7d-4b12-a329-a83b4e4ef04f',\n",
       " '157c3849-dd36-4793-a467-b121f84f1e69',\n",
       " '4fa20fca-89af-4396-b8f0-b94db86d9320',\n",
       " '62c3f5e5-5040-4eec-8dcc-d99999a81053',\n",
       " 'b3117b12-eb02-409d-ad6d-660a0fafcbb5',\n",
       " '66827755-2370-42f4-8703-fc361453af2a',\n",
       " '218a616c-33eb-4774-a1a3-5ef870af66db',\n",
       " 'c5c217f3-6a88-4e4e-9a09-d5d01d200d2c',\n",
       " '367f0e1a-456d-40eb-a52e-5e81e5b9164a',\n",
       " 'f2e1685f-bc9a-4071-bfd4-8b4bf706edea',\n",
       " '0868df75-6080-44e1-8ad0-e212e8b4c431',\n",
       " 'e09c0374-b995-44bf-9ae9-5ca5b853c37e',\n",
       " 'bd237b47-b8a8-4424-bbd4-ae4f6934f08c',\n",
       " '26d1cd70-c82c-4149-a440-8e5dd54e1a8c',\n",
       " 'e525c492-3609-423f-9e7a-8b9dcfad0197',\n",
       " 'b8063042-f747-4937-95df-b719e0ad938f',\n",
       " '11619f9d-12c2-4a2a-9cbb-498c2bc7c11a',\n",
       " 'a4517514-d0cf-43bd-adbd-fbb5a2f5dcf3',\n",
       " 'a0ba6250-631a-4d93-8c35-820da3c9ca53',\n",
       " '4749d080-71db-450d-b364-47fb497b9c47',\n",
       " 'c60e021e-d431-44d3-83e4-854c86b4346e',\n",
       " '9c7525ef-4173-4b48-a94e-83f80a53373f',\n",
       " '2a8495e6-8d6b-45ce-81ce-c95d56ff3021',\n",
       " '40b84c8a-090f-44db-bb48-126c5797d41f']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "uuids = [str(uuid4()) for _ in range(len(texts))]\n",
    "\n",
    "vector_store.add_documents(documents=texts, ids=uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* ing is to build intelligent systems that can reason\n",
      "and infer over natural language. The question an-\n",
      "swering (QA) task, in which models learn how to\n",
      "answer questions, is often used as a benchmark for\n",
      "quantitatively measuring the reasoning and infer-\n",
      "ring abilities of such intelligent systems.\n",
      "While many large-scale annotated general do-\n",
      "main QA datasets have been introduced (Ra-\n",
      "jpurkar et al., 2016; Lai et al., 2017; Ko ˇcisk`y\n",
      "Question:\n",
      "Do preoperative statins reduce atrial ﬁbrillation after\n",
      "coronary artery bypass grafting?\n",
      "Context:\n",
      "(Objective) Recent studies have demonstrated that statins\n",
      "have pleiotropic effects, including anti-inﬂammatory ef-\n",
      "fects and atrial ﬁbrillation (AF) preventive effects [...]\n",
      "(Methods) 221 patients underwent CABG in our hospital\n",
      "from 2004 to 2007. 14 patients with preoperative AF and\n",
      "4 patients with concomitant valve surgery [...]\n",
      "(Results) The overall incidence of postoperative AF was\n",
      "26%. Postoperative AF was signiﬁcantly lower in the [{'page': 0.0, 'source': 'Biomedical Research\\\\D19-1259.pdf'}]\n",
      "* ing is to build intelligent systems that can reason\n",
      "and infer over natural language. The question an-\n",
      "swering (QA) task, in which models learn how to\n",
      "answer questions, is often used as a benchmark for\n",
      "quantitatively measuring the reasoning and infer-\n",
      "ring abilities of such intelligent systems.\n",
      "While many large-scale annotated general do-\n",
      "main QA datasets have been introduced (Ra-\n",
      "jpurkar et al., 2016; Lai et al., 2017; Ko ˇcisk`y\n",
      "Question:\n",
      "Do preoperative statins reduce atrial ﬁbrillation after\n",
      "coronary artery bypass grafting?\n",
      "Context:\n",
      "(Objective) Recent studies have demonstrated that statins\n",
      "have pleiotropic effects, including anti-inﬂammatory ef-\n",
      "fects and atrial ﬁbrillation (AF) preventive effects [...]\n",
      "(Methods) 221 patients underwent CABG in our hospital\n",
      "from 2004 to 2007. 14 patients with preoperative AF and\n",
      "4 patients with concomitant valve surgery [...]\n",
      "(Results) The overall incidence of postoperative AF was\n",
      "26%. Postoperative AF was signiﬁcantly lower in the [{'page': 0.0, 'source': 'Biomedical Research\\\\D19-1259.pdf'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\n",
    "    \"Do preoperative statins reduce atrial ﬁbrillation after coronary artery bypass grafting?\",\n",
    "    k=2\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ing is to build intelligent systems that can reason\n",
      "and infer over natural language. The question an-\n",
      "swering (QA) task, in which models learn how to\n",
      "answer questions, is often used as a benchmark for\n",
      "quantitatively measuring the reasoning and infer-\n",
      "ring abilities of such intelligent systems.\n",
      "While many large-scale annotated general do-\n",
      "main QA datasets have been introduced (Ra-\n",
      "jpurkar et al., 2016; Lai et al., 2017; Ko ˇcisk`y\n",
      "Question:\n",
      "Do preoperative statins reduce atrial ﬁbrillation after\n",
      "coronary artery bypass grafting?\n",
      "Context:\n",
      "(Objective) Recent studies have demonstrated that statins\n",
      "have pleiotropic effects, including anti-inﬂammatory ef-\n",
      "fects and atrial ﬁbrillation (AF) preventive effects [...]\n",
      "(Methods) 221 patients underwent CABG in our hospital\n",
      "from 2004 to 2007. 14 patients with preoperative AF and\n",
      "4 patients with concomitant valve surgery [...]\n",
      "(Results) The overall incidence of postoperative AF was\n",
      "26%. Postoperative AF was signiﬁcantly lower in the\n"
     ]
    }
   ],
   "source": [
    "print(results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='485f6536-e41f-4f56-bb7e-9af909cc458a', metadata={'page': 4.0, 'source': 'Biomedical Research\\\\D19-1259.pdf'}, page_content='We ﬁne-tune BioBERT (Lee et al., 2019) on Pub-\\nMedQA as a baseline. BioBERT is initialized\\nwith BERT (Devlin et al., 2018) and further pre-\\ntrained on PubMed abstracts and PMC 7 articles.\\nExpectedly, it vastly outperforms BERT in vari-\\nous biomedical NLP tasks. We denote the original\\ntransformer weights of BioBERT as θ0.\\nWhile ﬁne-tuning, we feed PubMedQA ques-\\ntions and contexts (or long answers), separated\\n7https://www.ncbi.nlm.nih.gov/pmc/\\nby the special [SEP] token, to BioBERT. The\\nyes/no/maybe labels are predicted using the spe-\\ncial [CLS] embedding using a softmax function.\\nCross-entropy loss of predicted and true label dis-\\ntribution is denoted as LQA.')]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"k\": 1, \"score_threshold\": 0.5},\n",
    ")\n",
    "\n",
    "query = \"Advancements in CRISPR technology have opened new avenues for targeted gene editing, offering promising potential for treating genetic disorders.\"\n",
    "retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the HuggingFace API token as an environment variable\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "\n",
    "# Retrieve the environment variable to verify\n",
    "hugging_face_api_key = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "\n",
    "# Validate the token (simple check) \n",
    "if hugging_face_api_key is None or not hugging_face_api_key.startswith(\"hf_\"): \n",
    "    raise ValueError(\"Invalid HuggingFace API token.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "hf = HuggingFaceHub(\n",
    "    repo_id='mistralai/Mistral-7B-v0.1',\n",
    "    model_kwargs = {\"temperature\" : 0.1 , \"max_length\" : 500}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Advancements in CRISPR technology have opened new avenues for targeted gene editing, offering promising potential for treating genetic disorders.'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advancements in CRISPR technology have opened new avenues for targeted gene editing, offering promising potential for treating genetic disorders. However, the use of CRISPR in humans is still in its early stages, and there are several ethical and legal considerations that must be addressed before it can be widely adopted.\n",
      "\n",
      "One of the main ethical concerns surrounding the use of CRISPR in humans is the potential for unintended consequences. While CRISPR has the potential to correct genetic mutations that cause diseases, there is a risk that the technology could also introduce new mutations or alter the function of other genes.\n"
     ]
    }
   ],
   "source": [
    "output = hf.invoke(query)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. Please follow the following rules:\n",
    "1. If you don't know the answer, don't try to make up an answer. Just say \"I can't find the final answer but you may want to check the following links\".\n",
    "2. If you find the answer, write the answer in a concise way with five sentences maximum.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    " template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievalQA = RetrievalQA.from_chain_type(\n",
    "    llm = hf, \n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs = {\"prompt\" : PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USE THE RAG !!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to interact with your chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Advancements in CRISPR technology have opened new avenues for targeted gene editing, offering promising potential for treating genetic disorders.', 'result': 'Use the following pieces of context to answer the question at the end. Please follow the following rules:\\n1. If you don\\'t know the answer, don\\'t try to make up an answer. Just say \"I can\\'t find the final answer but you may want to check the following links\".\\n2. If you find the answer, write the answer in a concise way with five sentences maximum.\\n\\nWe ﬁne-tune BioBERT (Lee et al., 2019) on Pub-\\nMedQA as a baseline. BioBERT is initialized\\nwith BERT (Devlin et al., 2018) and further pre-\\ntrained on PubMed abstracts and PMC 7 articles.\\nExpectedly, it vastly outperforms BERT in vari-\\nous biomedical NLP tasks. We denote the original\\ntransformer weights of BioBERT as θ0.\\nWhile ﬁne-tuning, we feed PubMedQA ques-\\ntions and contexts (or long answers), separated\\n7https://www.ncbi.nlm.nih.gov/pmc/\\nby the special [SEP] token, to BioBERT. The\\nyes/no/maybe labels are predicted using the spe-\\ncial [CLS] embedding using a softmax function.\\nCross-entropy loss of predicted and true label dis-\\ntribution is denoted as LQA.\\n\\nQuestion: Advancements in CRISPR technology have opened new avenues for targeted gene editing, offering promising potential for treating genetic disorders.\\n\\nHelpful Answer:\\n\\n1. The question is asking about advancements in CRISPR technology and its potential for treating genetic disorders.\\n2. CRISPR is a gene editing technology that allows for precise and targeted modifications to DNA.\\n3. CRISPR technology has been used to treat genetic disorders such as sickle cell disease and cystic fibrosis.\\n4. CRISPR technology has also been used to modify the genomes of animals, such as pigs, to produce organs', 'source_documents': [Document(id='485f6536-e41f-4f56-bb7e-9af909cc458a', metadata={'page': 4.0, 'source': 'Biomedical Research\\\\D19-1259.pdf'}, page_content='We ﬁne-tune BioBERT (Lee et al., 2019) on Pub-\\nMedQA as a baseline. BioBERT is initialized\\nwith BERT (Devlin et al., 2018) and further pre-\\ntrained on PubMed abstracts and PMC 7 articles.\\nExpectedly, it vastly outperforms BERT in vari-\\nous biomedical NLP tasks. We denote the original\\ntransformer weights of BioBERT as θ0.\\nWhile ﬁne-tuning, we feed PubMedQA ques-\\ntions and contexts (or long answers), separated\\n7https://www.ncbi.nlm.nih.gov/pmc/\\nby the special [SEP] token, to BioBERT. The\\nyes/no/maybe labels are predicted using the spe-\\ncial [CLS] embedding using a softmax function.\\nCross-entropy loss of predicted and true label dis-\\ntribution is denoted as LQA.')]}\n"
     ]
    }
   ],
   "source": [
    "result = retrievalQA.invoke({\"query\" : query})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['query', 'result', 'source_documents'])\n"
     ]
    }
   ],
   "source": [
    "print(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 documents retrieved which are relevant to the query.\n",
      "****************************************************************************************************\n",
      "Relevant Document #1:\n",
      "Source file: Biomedical Research\\D19-1259.pdf, Page: 4.0\n",
      "Content: We ﬁne-tune BioBERT (Lee et al., 2019) on Pub-\n",
      "MedQA as a baseline. BioBERT is initialized\n",
      "with BERT (Devlin et al., 2018) and further pre-\n",
      "trained on PubMed abstracts and PMC 7 articles.\n",
      "Expectedly, it vastly outperforms BERT in vari-\n",
      "ous biomedical NLP tasks. We denote the original\n",
      "transformer weights of BioBERT as θ0.\n",
      "While ﬁne-tuning, we feed PubMedQA ques-\n",
      "tions and contexts (or long answers), separated\n",
      "7https://www.ncbi.nlm.nih.gov/pmc/\n",
      "by the special [SEP] token, to BioBERT. The\n",
      "yes/no/maybe labels are predicted using the spe-\n",
      "cial [CLS] embedding using a softmax function.\n",
      "Cross-entropy loss of predicted and true label dis-\n",
      "tribution is denoted as LQA.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "There are 1 documents retrieved which are relevant to the query.\n"
     ]
    }
   ],
   "source": [
    "relevant_docs = result['source_documents']\n",
    "print(f'There are {len(relevant_docs)} documents retrieved which are relevant to the query.')\n",
    "print(\"*\" * 100)\n",
    "for i, doc in enumerate(relevant_docs):\n",
    "    print(f\"Relevant Document #{i+1}:\\nSource file: {doc.metadata['source']}, Page: {doc.metadata['page']}\\nContent: {doc.page_content}\")\n",
    "    print(\"-\"*100)\n",
    "    print(f'There are {len(relevant_docs)} documents retrieved which are relevant to the query.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What are the methods used in the study?', 'result': 'Use the following pieces of context to answer the question at the end. Please follow the following rules:\\n1. If you don\\'t know the answer, don\\'t try to make up an answer. Just say \"I can\\'t find the final answer but you may want to check the following links\".\\n2. If you find the answer, write the answer in a concise way with five sentences maximum.\\n\\nin Fig. 3. Nearly all instances are human studies\\nand they cover a wide variety of topics, including\\nretrospective, prospective, and cohort studies, dif-\\nferent age groups, and healthcare-related subjects\\nlike treatment outcome, prognosis and risk factors\\nof diseases.\\n6https://www.nlm.nih.gov/mesh\\nQuestion TypeReasoning TypeNumber interpretationalreadyin context?\\nFigure 4: Proportional relationships between corre-\\nsponded question types, reasoning types, and whether\\nthe text interpretations of numbers exist in contexts.\\nQuestion and Reasoning Types: We sampled\\n200 examples from PQA-L and analyzed the types\\nof questions and types of reasoning required to an-\\nswer them, which is summarized in Table 3. Var-\\nious types of questions have been asked, includ-\\ning causal effects, evaluations of therapies, relat-\\nedness, and whether a statement is true. Besides,\\nPubMedQA also covers several different reason-\\ning types: most (57.5%) involve comparing multi-\\n\\nQuestion: What are the methods used in the study?\\n\\nHelpful Answer:\\n\\nThe study used a retrospective cohort design to\\nexamine the association between the use of\\nantidepressants and the risk of developing\\nbreast cancer. The study included 110,430 women\\nwho were enrolled in the Nurses\\' Health Study\\nII and were followed for a median of 10 years.\\nThe study found that women who used antide-\\npressants had a higher risk of developing breast\\ncancer', 'source_documents': [Document(id='6835e416-248f-4687-9716-3c3b0eeb46c2', metadata={'page': 3.0, 'source': 'Biomedical Research\\\\D19-1259.pdf'}, page_content='in Fig. 3. Nearly all instances are human studies\\nand they cover a wide variety of topics, including\\nretrospective, prospective, and cohort studies, dif-\\nferent age groups, and healthcare-related subjects\\nlike treatment outcome, prognosis and risk factors\\nof diseases.\\n6https://www.nlm.nih.gov/mesh\\nQuestion TypeReasoning TypeNumber interpretationalreadyin context?\\nFigure 4: Proportional relationships between corre-\\nsponded question types, reasoning types, and whether\\nthe text interpretations of numbers exist in contexts.\\nQuestion and Reasoning Types: We sampled\\n200 examples from PQA-L and analyzed the types\\nof questions and types of reasoning required to an-\\nswer them, which is summarized in Table 3. Var-\\nious types of questions have been asked, includ-\\ning causal effects, evaluations of therapies, relat-\\nedness, and whether a statement is true. Besides,\\nPubMedQA also covers several different reason-\\ning types: most (57.5%) involve comparing multi-')]}\n"
     ]
    }
   ],
   "source": [
    "# Function to chat with your model\n",
    "def chat_with_model(question):\n",
    "    response = retrievalQA.invoke({\"query\": question})\n",
    "    return response\n",
    "\n",
    "# Example interaction\n",
    "user_question = \"What are the methods used in the study?\"\n",
    "print(chat_with_model(user_question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
